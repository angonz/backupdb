#!/bin/bash
# Add to crontab with 'crontab -e', adding the following line at the bottom (remove the initial '#'):
# 00 7    * * *    /home/ubuntu/backupdb/backupdb >> /home/ubuntu/backupdb.log
# This will run the script at 7am GMT every day and append the results to /home/ubuntu/backupdb.log

MONGO_USER=$(awk '/^EDXAPP_MONGO_USER/{print $2}' ~/my-passwords.yml | tr -d "'")
MONGO_PASS=$(awk '/^EDXAPP_MONGO_PASSWORD/{print $2}' ~/my-passwords.yml | tr -d "'")
FORUM_MONGO_USER=$(awk '/^FORUM_MONGO_USER/{print $2}' ~/my-passwords.yml | tr -d '"')
FORUM_MONGO_PASS=$(awk '/^FORUM_MONGO_PASSWORD/{print $2}' ~/my-passwords.yml | tr -d "'")
# AWS S3 access key and token
AWS_S3_ACCESS_KEY_ID=$(awk '/EDXAPP_AWS_ACCESS_KEY_ID/{print $2}' /edx/app/edx_ansible/server-vars.yml  | tr -d "\"")
AWS_S3_SECRET_ACCESS_KEY=$(awk '/EDXAPP_AWS_SECRET_ACCESS_KEY/{print $2}' /edx/app/edx_ansible/server-vars.yml  | tr -d "\"")
AWS_URL=s3-sa-east-1.amazonaws.com #If s3.amazonaws.com redirect does not work, use region specific url
BUCKET=aulasneo-backup
TARGETDIR=/home/ubuntu/edxdbbackup

NOW=`date +"%Y-%m-%d"`
FILEPATH=/home/ubuntu/
FILENAME=edxdbbackup_`hostname`_$NOW.tar.gz
RESOURCE="/$BUCKET/$FILENAME"
CONTENT_TYPE="application/x-compressed-tar"
DATE_VALUE=`date -R`
STRING_TO_SIGN="PUT\n\n$CONTENT_TYPE\n$DATE_VALUE\n$RESOURCE"
SIGNATURE=`echo -en $STRING_TO_SIGN | openssl sha1 -hmac $AWS_S3_SECRET_ACCESS_KEY -binary | base64`

if [ -d $TARGETDIR ]; then
  rm -R $TARGETDIR
fi
mkdir $TARGETDIR

# backing up mysql db
echo "Backing up mysql to $TARGETDIR/backup.sql"
mysqldump --all-databases --add-drop-database --single-transaction -u root > $TARGETDIR/backup.sql

# backing up mongo db
echo "Backing up mongodb edxapp to $TARGETDIR"
mongodump --username $MONGO_USER --password $MONGO_PASS --db edxapp --out $TARGETDIR
echo "Backing up mongodb cs_comments_service to $TARGETDIR"
mongodump --username $FORUM_MONGO_USER --password $FORUM_MONGO_PASS --db cs_comments_service --out $TARGETDIR

# Packing it to single file for easy copying to second sever
echo "packing $TARGETDIR into $FILENAME"
tar -zvcf $FILEPATH$FILENAME $TARGETDIR
FILESIZE=`stat --printf="%s" $FILEPATH$FILENAME`
echo "targz size: $FILESIZE bytes"

rm -R $TARGETDIR

# Upload to AWS S3 bucket 
# See http://tmont.com/blargh/2014/1/uploading-to-s3-in-bash

echo "Uploading backup to AWS S3 bucket $BUCKET"

curl -X PUT -T "$FILEPATH$FILENAME" \
        -H "Host: $BUCKET.$AWS_URL" \
        -H "Date: $DATE_VALUE" \
        -H "Content-Type: $CONTENT_TYPE" \
        -H "Authorization: AWS $AWS_S3_ACCESS_KEY_ID:$SIGNATURE" \
        https://$BUCKET.$AWS_URL/$FILENAME


